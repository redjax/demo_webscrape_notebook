{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set notebook to auto reload updated modules. Put this at the top of your Jupyter\n",
    "#  notebooks, and if you import code from local .py files, you won't have to restart\n",
    "#  the kernel when you make changes in the code.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Book Scraper\n",
    "\n",
    "This notebook scrapes the [books.toscrape.com site](https://books.toscrape.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import packages to use in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from hishel import SyncSqliteStorage\n",
    "from hishel.httpx import SyncCacheTransport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Define constants for the notebook (values that are set at the beginning and don't change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path to HTTP cache database\n",
    "CACHE_PATH = \".cache/http.cache\"\n",
    "\n",
    "## Base URL for target site\n",
    "BASE_URL: str = \"https://books.toscrape.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Re-usable code for the notebook to call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_transport(\n",
    "    cache_file: str | Path = CACHE_PATH, ttl: int = 900\n",
    ") -> httpx.HTTPTransport:\n",
    "    \"\"\"Return a Hishel SyncCacheTransport with SQLite cache.\n",
    "\n",
    "    Params:\n",
    "        cache_file (str | Path): Path to a SQLite file where responses will be cached.\n",
    "        ttl (int): Time an object should live in the cache before being cleared/refreshed (default=900, which is 15 minutes).\n",
    "    \"\"\"\n",
    "    ## Ensure cache parent directory exists. If it already exists, nothing will happen\n",
    "    Path(cache_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ## Prepare a Hishel SQLite database to use as a cache\n",
    "    storage = SyncSqliteStorage(\n",
    "        database_path=str(cache_file),\n",
    "        default_ttl=ttl,\n",
    "        refresh_ttl_on_access=True,\n",
    "    )\n",
    "\n",
    "    ## Create the cache transport\n",
    "    return SyncCacheTransport(\n",
    "        next_transport=httpx.HTTPTransport(),\n",
    "        storage=storage,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client(\n",
    "    use_cache: bool = False, cache_file: str | Path = CACHE_PATH, ttl: int = 900\n",
    ") -> httpx.Client:\n",
    "    \"\"\"Return a reusable HTTPX client optionally with cache transport.\n",
    "\n",
    "    Params:\n",
    "        use_cache (bool): Whether to enable Hishel SQLite caching (default=False).\n",
    "        ttl (int): Time an object should live in the cache before being cleared/refreshed (default=900, which is 15 minutes).\n",
    "        cache_file (str | Path): Path to a SQLite file where responses will be cached.\n",
    "    \"\"\"\n",
    "    ## Creates a cache transport for HTTPX if use_cache=True, otherwise value is None\n",
    "    transport: SyncCacheTransport = (\n",
    "        get_cache_transport(cache_file, ttl) if use_cache else None\n",
    "    )\n",
    "\n",
    "    ## Create an HTTPX client with optional cache transport\n",
    "    return httpx.Client(transport=transport, timeout=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Request page HTML & create BeautifulSoup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create HTTP client to use for requests. Add a cache so we're not repeatedly sending live requests\n",
    "http_client: httpx.Client = get_client(use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do a HEAD request to check if the site is online (should get a 200 response)\n",
    "#  Raise an exception for any non-successful response, i.e. 400, 404, etc\n",
    "ping: httpx.Response = http_client.head(BASE_URL)\n",
    "ping.raise_for_status()\n",
    "\n",
    "display(ping.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of using httpx.get() directly, you can also pre-create a Request object and use the client to send it\n",
    "homepage_request: httpx.Request = httpx.Request(method=\"GET\", url=BASE_URL)\n",
    "display(f\"Homepage request URL: {homepage_request.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of httpx.get(), use httpx.send() and give it the homepage_request object\n",
    "\n",
    "homepage_res: httpx.Response = http_client.send(homepage_request)\n",
    "homepage_res.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decode the response content into a string with the page's URL\n",
    "homepage_html: str = homepage_res.content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a 'soup' from the HTML response\n",
    "homepage_soup: BeautifulSoup = BeautifulSoup(homepage_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You can use .prettify() to format the HTML, which is useful for saving the HTML to a file.\n",
    "\n",
    "You can open this file to find the tags you want to scrape, or use it as a cache, only sending\n",
    "a request if this file does not exist (otherwise reading from the saved file).\n",
    "\n",
    "The code below just saves the file for demonstration purposes, the response is already cached in the\n",
    "path defined in the CACHE_PATH constant.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"homepage.html\", \"w\") as f:\n",
    "    f.write(homepage_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Scrape the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_title = homepage_soup.title\n",
    "\n",
    "## Display the <title> tag & text\n",
    "display(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display just the text\n",
    "display(page_title.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the sidebar contents (i.e. genre links)\n",
    "sidebar_genres_div = homepage_soup.find(\"div\", class_=\"side_categories\")\n",
    "\n",
    "display(type(sidebar_genres_div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the <ul> nav list from the sidebar div\n",
    "sidebar_links_ul = sidebar_genres_div.find(\"ul\", class_=\"nav nav-list\")\n",
    "\n",
    "all_links = []\n",
    "\n",
    "## Extract text & link from all <li> elements in the <ul>\n",
    "for li in sidebar_links_ul.find_all(\"li\"):\n",
    "    link = li.find(\"a\")\n",
    "\n",
    "    if link:\n",
    "        ## Extract the href= link\n",
    "        href = link.get(\"href\")\n",
    "        ## Extract the linked text\n",
    "        text = link.get_text(strip=True)  # strip=True removes things like \\n characters\n",
    "\n",
    "        ## Add dict with link text & href to all_links list\n",
    "        all_links.append({\"text\": text, \"href\": href})\n",
    "\n",
    "display(f\"Found [{len(all_links)}] link(s) in sidebar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a random link from the list\n",
    "rand_index = random.randint(0, len(all_links) - 1)\n",
    "rand_link = all_links[rand_index]\n",
    "\n",
    "display(f\"Example link: {rand_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find all books by getting <article class=\"product_pod\"> tags\n",
    "products = homepage_soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "display(f\"Found [{len(products)}] product(s) on the page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List to hold scraped books\n",
    "books_data = []\n",
    "\n",
    "## Iterate over products found on page\n",
    "for product in products:\n",
    "    ## Extract book title and link (from h3 > a)\n",
    "    title_link = product.find(\"h3\").find(\"a\")\n",
    "    title = title_link.get_text(strip=True)\n",
    "\n",
    "    ## Extract link to book\n",
    "    book_url = title_link.get(\"href\")\n",
    "\n",
    "    ## Extract book Price\n",
    "    price = product.find(\"p\", class_=\"price_color\").get_text(strip=True)\n",
    "\n",
    "    ## Extract rating (class=\"star-rating\" on element <p>)\n",
    "    rating_elem = product.find(\"p\", class_=\"star-rating\")\n",
    "    rating = rating_elem.get(\"class\")[1] if rating_elem else \"Unknown\"\n",
    "\n",
    "    ## Extract stock availability\n",
    "    availability = product.find(\"p\", class_=\"instock availability\").get_text(strip=True)\n",
    "\n",
    "    ## Add book data to list\n",
    "    books_data.append(\n",
    "        {\n",
    "            \"title\": title,\n",
    "            \"url\": book_url,\n",
    "            \"price\": price,\n",
    "            \"rating\": rating,\n",
    "            \"availability\": availability,\n",
    "        }\n",
    "    )\n",
    "\n",
    "display(f\"Found [{len(books_data)}] book(s) on page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_book_index = random.randint(0, len(books_data) - 1)\n",
    "display(f\"Example book: {books_data[rand_book_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_books = []\n",
    "\n",
    "## Clean book data, i.e. extract currency symbol & amount\n",
    "for book in books_data:\n",
    "    ## Strip currency symbol\n",
    "    price_text = book[\"price\"].strip()\n",
    "\n",
    "    ## Match currency symbols to strings\n",
    "    currency_symbol = price_text[0]\n",
    "\n",
    "    book[\"currency_symbol\"] = currency_symbol\n",
    "\n",
    "    match currency_symbol:\n",
    "        case \"£\":\n",
    "            book[\"currency\"] = \"GBP\"\n",
    "        case \"$\":\n",
    "            book[\"currency\"] = \"USD\"\n",
    "        case \"€\":\n",
    "            book[\"currency\"] = \"EUR\"\n",
    "        case \"¥\":\n",
    "            book[\"currency\"] = \"JPY\"\n",
    "        case _:\n",
    "            book[\"currency\"] = \"Unknown\"\n",
    "\n",
    "    ## Extract numeric price value, i.e. 51.77\n",
    "    book[\"price_numeric\"] = float(book[\"price\"][1:].replace(\",\", \"\").strip())\n",
    "\n",
    "    cleaned_books.append(book)\n",
    "\n",
    "display(f\"Cleaned [{len(cleaned_books)}] book(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a random cleaned book\n",
    "rand_cleaned_book_index = random.randint(0, len(cleaned_books) - 1)\n",
    "display(f\"Example cleaned book: {cleaned_books[rand_cleaned_book_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert books data to JSON and save\n",
    "with open(\"books_data.json\", \"w\") as f:\n",
    "    ## Dump Python list of dicts to JSON string\n",
    "    #  Note that currency symbols will be encoded, i.e. £ becomes \\u00a3\n",
    "    _data = json.dumps(cleaned_books, indent=2, default=str)\n",
    "\n",
    "    f.write(_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload file to demonstrate currency symbol decoding\n",
    "with open(\"books_data.json\", \"r\") as f:\n",
    "    _data = f.read()\n",
    "\n",
    "    _json = json.loads(_data)\n",
    "\n",
    "display(type(_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a random book from the loaded JSON to show currency symbol decoding\n",
    "rand_cleaned_book_index = random.randint(0, len(cleaned_books) - 1)\n",
    "display(\"Currency symbol (decoded from being stored as unicode):\")\n",
    "display(f\"{cleaned_books[rand_cleaned_book_index].get('currency_symbol')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Other ideas\n",
    "\n",
    "That about does it for the basics of scraping with BeautifulSoup. There are many other things you could do next, like converting the dictionaries to Python classes, encapsulating the 'cleanup' code in a function you can call repeatedly on lines of text, saving the scraped data to a database (i.e. with [SQLAlchemy](https://sqlalchemy.org) or the stdlib `sqlite3` library), etc.\n",
    "\n",
    "You could also add more scrapes, like finding the 'next' button and iterating over pages, scraping each one for books. You could implement opening each individual book and scraping the data on that page (a new request), or iterate over every link in the sidebar (\"crawling\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close HTTP client\n",
    "http_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
